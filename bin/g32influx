#!/usr/bin/env python3

# Read from .g3 file and insert into InfluxDB.

import argparse
import datetime
import hashlib
import logging
import numpy as np
import os

from tqdm import tqdm

from influxdb import InfluxDBClient
from influxdb.exceptions import InfluxDBClientError

from ocs.checkdata import _build_file_list
from so3g import hk

# Accepted formats for parsing the g3 file datetimes.
_G3_DATETIME_FMT = ["%Y-%m-%d-%H-%M-%S.g3",
                    "%Y-%m-%d_T_%H:%M:%S.g3"]

# Measurement name in influx DB for logging which files have been/are being
# transfered over.
_LOG_MEASUREMENT = ".g32influx.log"

def _md5sum(path, blocksize=65536):
    """Compute md5sum of a file.

    References
    ----------
    - https://stackoverflow.com/questions/3431825/generating-an-md5-checksum-of-a-file

    Parameters
    ----------
    path : str
        Full path to file for which we want the md5
    blocksize : int
        blocksize we want to read the file in chunks of to avoid fitting the
        whole file into memory. Defaults to 65536

    Returns
    -------
    str
        Hex string representing the md5sum of the file

    """
    hash_ = hashlib.md5()
    with open(path, "rb") as f:
        for block in iter(lambda: f.read(blocksize), b""):
            hash_.update(block)
    return hash_.hexdigest()


def _get_file_list(target):
    """Build list of files to scan.
    This is based on ocs.checkdata._build_file_list, but copied here so that if
    this script is removed from the ocs repo we don't have to import the latter
    just for one method.

    Parameters
    ----------
    target : str
        File or directory to scan.

    Returns
    -------
    list
        List of full paths to files for scanning.

    """
    _file_list = []
    if os.path.isfile(target):
        _file_list.append(target)
    elif os.path.isdir(target):
        a = os.walk(target)
        for root, _, _file in a:
            for g3 in _file:
                if g3[-2:] == "g3":
                    _file_list.append(os.path.join(root, g3))

    return _file_list

class DataLoader:
    """Load data from .g3 file into an InfluxDB instance.

    Parameters
    ----------
    target : str
        File or directory to scan.
    host : str
        InfluxDB host address
    port : int
        InfluxDB port

    Attributes
    ----------
    target : str
        File or directory to scan.
    db : influxdb.InfluxDBClient
        Connection to the InfluxDB, used to publish data to the database.
    """
    def __init__(self, target, host='localhost', port=8086,
                 startdate="1970-01-01", enddate="2070-01-01", force=False):
        self.db = InfluxDBClient(host=host, port=port)
        self._init_influxdb()

        self.target = os.path.abspath(target)
        self._file_list = _get_file_list(self.target)

        self.startdate = datetime.datetime.strptime(startdate, "%Y-%m-%d")
        self.enddate = datetime.datetime.strptime(enddate, "%Y-%m-%d")
        self.force = force

        self._file_list = self._reduce_filelist_by_date()

    def _init_influxdb(self, db='ocs_feeds'):
        """Initializes InfluxDB after connection.

        Gets a list of existing databases within InfluxDB, creates db if it
        doesn't exist (defaults to 'ocs_feeds'), and switches the client to
        that db.

        Parameters
        ----------
        db : str
            Name for the database, default to 'ocs_feeds'.

        """
        db_list = self.db.get_list_database()
        db_names = [x['name'] for x in db_list]

        if 'ocs_feeds' not in db_names:
            logging.info("ocs_feeds DB doesn't exist, creating DB")
            self.db.create_database(db)

        self.db.switch_database(db)

    def _reduce_filelist_by_date(self):
        """If the user has passed in optional start and end date parameters,
        limit the filelist by removing files that fall outside of the given
        range.
        """
        new_list = []
        for f in self._file_list:
            date_string = os.path.basename(f)
            dt = None
            for fmt in _G3_DATETIME_FMT: 
                try:
                    dt = datetime.datetime.strptime(date_string, fmt)
                except ValueError:
                    pass
            if dt == None:
                logging.error(f"Removing {f} from file list: bad filename "\
                               "format.")
            elif dt > self.startdate and dt < self.enddate:
                new_list.append(f)
            else:
                logging.info(f"Removing {f} from file list: "\
                              "outside of start/end dates.")

        return new_list

    def run(self):
        for path in self._file_list:
            md5sum = _md5sum(path)
            fname = os.path.basename(path)
            res = self.db.query("SELECT * FROM \"%s\" WHERE "\
                                "md5sum = '%s' AND status = 'completed';" %\
                                (_LOG_MEASUREMENT, md5sum))
            if len(res.items()) and not self.force:
                logging.info("Skipping %s: already completed." % fname)
            else:
                if len(res.items()):
                    logging.info("Rewriting %s due to `--force`." % fname)
                try:
                     scanner = SingleFileScanner(path, self.db, md5sum=md5sum)
                     scanner.run()
                except RuntimeError:
                     logging.error("Unable to process file, skipping.")


class SingleFileScanner:
    """Object for scanning and publishing a single .g3 file.

    Since we want to track which files are being uploaded so that an upload can
    be resumed if interrupted it's perhaps the simplest to upload them
    individually. While this doesn't take advantage of the nice
    so3g.hk.HKArchiveScanner functionality of reading multiple files, or time
    limiting step is actually pushing data into the InfluxDB.

    Parameters
    ----------
    path : str
        Full path to file for scanning
    md5sum : str
        Computed md5sum for file. (If `None` is passed, it will be computed.)
    db : influxdb.InfluxDBClient
        Connection to the InfluxDB, used to publish data to the database.

    Attributes
    ----------
    path : str
        Full path to file for scanning
    md5sum : str
        The md5sum of the file.
    db : influxdb.InfluxDBClient
        Connection to the InfluxDB, used to publish data to the database.
    hkas : so3g.hk.HKArchiveScanner
        HKArchiveScanner for reading in the data
    cat :
        Finalized HKArchiveScanner
    fields
        Fields within the file as returned by cat.get_fields()
    timelines
        Timelines within the file as returned by cat.get_fields()

    """
    def __init__(self, path, db, md5sum=None):
        self.path = path
        self.dir, self.basename = os.path.split(path)
        self.md5sum = md5sum if md5sum is not None else _md5sum(path)
        self.db = db

        self.hkas = hk.HKArchiveScanner()
        self.cat = None

        self.fields = None
        self.timelines = None

    def scan_file(self):
        """Scan the file with the HKArchiveScanner and get the fields
        for later processing.

        """
        self.hkas.process_file(self.path)
        self.cat = self.hkas.finalize()
        self.fields, self.timelines = self.cat.get_fields()

        return 0

    def format_field(self, field):
        """Format a given field for publishing to the database.
        Deprecated: formating field by field is significantly slower than doing
        it by timeline: see `format_timeline()`.

        Parameters
        ----------
        field : str
            Field to publish data from, will query the finalized HKArchive

        Returns
        -------
        list
            List of values formatted for writing to InfluxDB

        """
        t, x = self.cat.simple(field)
        logging.debug("field:", field)
        agent_address = field.split(".feeds.")[0].replace(" ", "\\ ")
        ff = field.split(".feeds.")[1]
        feed_tag = ff.split(".")[0].replace(" ", "\\ ")
        field = ff.split(".")[1].replace(" ", "\\ ")

        line = []
        for _x, _t in zip(x, t):
            line.append("%s,feed=%s %s=%s %d\n" % (agent_address, feed_tag,
                                                   field, _x, _t * 1e9))
        return line

    def _influx_log(self, status, msg=None):
        """Write a log message to influx about current file.

        Parameters
        ----------
        status : str
            The status to log.
        msg : str
            Any message (e.g., an error message) to log.
        """
        l = "%s,dir=%s,file=%s,md5sum=%s status=\"%s\"" %\
            (_LOG_MEASUREMENT, self.dir, self.basename, self.md5sum, status)
        if msg:
            l += ",msg=\"%s\"" % msg
        try:
            self.db.write_points([l], protocol="line")
        except InfluxDBClientError as e:
            logging.error("Error writing log: %s." % e)
 
    def format_timeline(self, timeline, name):
        """Format a given timeline for publishing to the database.

        Parameters
        ----------
        field : str
            Timeline to publish data from, will query the finalized HKArchive.

        Returns
        -------
        list
            List of points, in line protocol, for writing to InfluxDB.
        """
        field = timeline["field"]
        logging.debug("Timeline: %s" % name)

        # Do some good, ol' fashioned parsing.
        s = field[0].split(".feeds.")
        agent_address = s[0].replace(" ", "\ ")
        ff = field[0].split(".feeds.")[1]
        feed_val = s[1].split(".")[0].replace(" ", "\ ")
        field_tag = [f.split(".feeds.")[1].split(".")[1].replace(" ", "\ ") \
                     for f in field]

        # Get the data. Since all the fields belong to the same timeline, their
        # timestamps are all the same object, which we can just grab from the 
        # first element of returned array. Then we need to transpose the data 
        # so that we can read it timestamp by timestamp.
        raw_data = self.cat.simple(field)
        t = raw_data[0][0]
        data = np.transpose(np.stack([d[1] for d in raw_data]))

        # Create the line output.
        line = []
        for _t, _data in zip(t, data):
            flist = ",".join(["%s=%s" % (_f, _d) \
                              for _f, _d in zip(field_tag, _data)])
            line.append("%s,feed=%s,g3_md5sum=%s %s %d\n" %\
                        (agent_address, feed_val, self.md5sum, flist, _t * 1e9))
        return line

    def publish_file(self, batch_size=100000):
        """Publish a files contents to InfluxDB.

        Parameters
        ----------
        batch_size : int
            Number of points to publish per write, passed to
            influxdb.write_points(). Defaults to 100,000, which seems
            reasonable.

        Returns
        -------
        int
            0 if good, 2 if an excpetion occurred during publishing
        """
        return_value = 0
        self._influx_log("started")

        for name, timeline in tqdm(self.timelines.items(),
                                   desc=f"{self.basename} Timelines"):
            try:
                payload = self.format_timeline(timeline, name)
            except ValueError:
                logging.error("Unable to format payload properly, possibly " +
                              "trying to process old .g3 file format...")
                self._influx_log("error", "unable to format payload")
                self._influx_log("finished")
                return 2
            try:
                self.db.write_points(payload, batch_size=batch_size,
                                     protocol="line")
            except InfluxDBClientError as e:
                logging.error(f"ERROR in {self.path}")
                logging.error(f"client error, likely a type error: {e}")
                logging.debug(f"payload: {payload}")
                self._influx_log("error", "unable to write to influx")
                return_value = 2

        if not return_value:
            self._influx_log("completed")
        self._influx_log("finished")
        return return_value

    def run(self):
        self.scan_file()
        pub_ret = self.publish_file()

        return pub_ret


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('target', help='File or directory to scan.')
    parser.add_argument('host', help='InfluxDB host.')
    parser.add_argument('port', help='InfluxDB port.')
    parser.add_argument('--start', '-s', default='1970-01-01',
                        help='Set startdate, cutting all files that start '\
                             'before this date.')
    parser.add_argument('--end', '-e', default='2070-01-01',
                        help='Set enddate, cutting all files that start '\
                        'after this date.')
    parser.add_argument('--log', '-l', default='WARNING',
                        help='Set loglevel.')
    parser.add_argument('--logfile', '-L', default='g32influx.log',
                        help='Set the logfile.')
    parser.add_argument('--force', '-f', action='store_true',
                        help='If a file has already been written to influx,'\
                             'force a rewrite.')
    # parser.add_argument('--docker', '-d', action='store_true',
    #                     help='Force use of docker, even if so3g is \
    #                            installed.')
    args = parser.parse_args()

    # Logging Configuration
    numeric_level = getattr(logging, args.log.upper(), None)
    if not isinstance(numeric_level, int):
        raise ValueError('Invalid log level: %s' % loglevel)
    logging.basicConfig(filename=args.logfile, level=numeric_level)

    dl = DataLoader(args.target, host=args.host, port=args.port,
                    startdate=args.start, enddate=args.end, force=args.force)
    dl.run()


if __name__ == "__main__":
    main()
